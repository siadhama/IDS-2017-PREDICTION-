{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10574,"status":"ok","timestamp":1759564187568,"user":{"displayName":"bt23103127 Aniket Harit","userId":"13209463969149351825"},"user_tz":-330},"id":"PCg_U6t6apuE","outputId":"4454911c-7b87-4e39-c6ae-2dae00ef6ec6"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-10-04 07:49:36--  http://cicresearch.ca/CICDataset/CIC-IDS-2017/Dataset/CIC-IDS-2017/CSVs/MachineLearningCSV.zip\n","Resolving cicresearch.ca (cicresearch.ca)... 205.174.165.80\n","Connecting to cicresearch.ca (cicresearch.ca)|205.174.165.80|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 235102953 (224M) [application/zip]\n","Saving to: ‘MachineLearningCSV.zip’\n","\n","MachineLearningCSV. 100%[===================>] 224.21M  31.5MB/s    in 10s     \n","\n","2025-10-04 07:49:47 (22.4 MB/s) - ‘MachineLearningCSV.zip’ saved [235102953/235102953]\n","\n"]}],"source":["# Use the wget command to download the ZIP file containing all CSVs\n","!wget http://cicresearch.ca/CICDataset/CIC-IDS-2017/Dataset/CIC-IDS-2017/CSVs/MachineLearningCSV.zip\n","\n","# The file will be saved in your current working directory in the Colab instance, which is typically /content/"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11731,"status":"ok","timestamp":1759564201064,"user":{"displayName":"bt23103127 Aniket Harit","userId":"13209463969149351825"},"user_tz":-330},"id":"ThXipOHVa-u6","outputId":"66b59e98-c145-4351-958b-916e9d4eaaef"},"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  MachineLearningCSV.zip\n","   creating: MachineLearningCVE/\n","  inflating: MachineLearningCVE/Wednesday-workingHours.pcap_ISCX.csv  \n","  inflating: MachineLearningCVE/Tuesday-WorkingHours.pcap_ISCX.csv  \n","  inflating: MachineLearningCVE/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv  \n","  inflating: MachineLearningCVE/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv  \n","  inflating: MachineLearningCVE/Monday-WorkingHours.pcap_ISCX.csv  \n","  inflating: MachineLearningCVE/Friday-WorkingHours-Morning.pcap_ISCX.csv  \n","  inflating: MachineLearningCVE/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv  \n","  inflating: MachineLearningCVE/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv  \n"]}],"source":[" # Unzip the downloaded file\n","!unzip MachineLearningCSV.zip"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1759568387787,"user":{"displayName":"bt23103127 Aniket Harit","userId":"13209463969149351825"},"user_tz":-330},"id":"sgRL1CIIbFx3","outputId":"ef33b8c8-df04-4953-ded2-581123875dd0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 8 files to process.\n"]}],"source":["import pandas as pd\n","import os\n","\n","# Define the base directory where the CSV files are located\n","data_dir = '/content/MachineLearningCVE/'\n","\n","# Get a list of all CSV files in that directory\n","# We use endswith('.csv') to ensure we only target the data files.\n","all_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.csv')]\n","\n","print(f\"Found {len(all_files)} files to process.\")\n","# print(all_files) # Uncomment this to see the list of file paths"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39985,"status":"ok","timestamp":1759568430706,"user":{"displayName":"bt23103127 Aniket Harit","userId":"13209463969149351825"},"user_tz":-330},"id":"YlXJOpevb1Wl","outputId":"aecf04af-6e35-4b17-8988-b65d23d034f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 8 files to process.\n","\n","Loaded: Monday-WorkingHours.pcap_ISCX.csv with shape (529918, 79)\n","Loaded: Wednesday-workingHours.pcap_ISCX.csv with shape (692703, 79)\n","Loaded: Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv with shape (170366, 79)\n","Loaded: Tuesday-WorkingHours.pcap_ISCX.csv with shape (445909, 79)\n","Loaded: Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv with shape (225745, 79)\n","Loaded: Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv with shape (286467, 79)\n","Loaded: Friday-WorkingHours-Morning.pcap_ISCX.csv with shape (191033, 79)\n","Loaded: Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv with shape (288602, 79)\n","\n","--- Concatenation Complete ---\n","Final shape of the raw master DataFrame: (2830743, 79)\n","\n","--- Data Clean-up Complete ---\n","Number of missing values (NaN/Inf) remaining: 0\n","Final shape of the cleaned master DataFrame: (2830743, 79)\n","\n","First 5 rows of the combined DataFrame:\n","   Destination Port  Flow Duration  Total Fwd Packets  Total Backward Packets  \\\n","0             49188              4                  2                       0   \n","1             49188              1                  2                       0   \n","2             49188              1                  2                       0   \n","3             49188              1                  2                       0   \n","4             49486              3                  2                       0   \n","\n","   Total Length of Fwd Packets  Total Length of Bwd Packets  \\\n","0                           12                            0   \n","1                           12                            0   \n","2                           12                            0   \n","3                           12                            0   \n","4                           12                            0   \n","\n","   Fwd Packet Length Max  Fwd Packet Length Min  Fwd Packet Length Mean  \\\n","0                      6                      6                     6.0   \n","1                      6                      6                     6.0   \n","2                      6                      6                     6.0   \n","3                      6                      6                     6.0   \n","4                      6                      6                     6.0   \n","\n","   Fwd Packet Length Std  ...  min_seg_size_forward  Active Mean  Active Std  \\\n","0                    0.0  ...                    20          0.0         0.0   \n","1                    0.0  ...                    20          0.0         0.0   \n","2                    0.0  ...                    20          0.0         0.0   \n","3                    0.0  ...                    20          0.0         0.0   \n","4                    0.0  ...                    20          0.0         0.0   \n","\n","   Active Max  Active Min  Idle Mean  Idle Std  Idle Max  Idle Min   Label  \n","0           0           0        0.0       0.0         0         0  BENIGN  \n","1           0           0        0.0       0.0         0         0  BENIGN  \n","2           0           0        0.0       0.0         0         0  BENIGN  \n","3           0           0        0.0       0.0         0         0  BENIGN  \n","4           0           0        0.0       0.0         0         0  BENIGN  \n","\n","[5 rows x 79 columns]\n"]}],"source":["import pandas as pd\n","import os\n","import numpy as np\n","\n","# Define the base directory where the CSV files are located (using your provided path)\n","data_dir = '/content/MachineLearningCVE/'\n","\n","# Get a list of all CSV files in that directory\n","all_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.csv')]\n","\n","print(f\"Found {len(all_files)} files to process.\\n\")\n","\n","# Initialize an empty list to hold the DataFrames\n","list_of_dfs = []\n","\n","# --- 1. Read and Concatenate ---\n","for file_path in all_files:\n","    try:\n","        # Read the CSV file\n","        # The 'low_memory=False' argument is often needed for large network traffic datasets\n","        df_temp = pd.read_csv(file_path, low_memory=False)\n","\n","        # Add the temporary DataFrame to our list\n","        list_of_dfs.append(df_temp)\n","\n","        print(f\"Loaded: {os.path.basename(file_path)} with shape {df_temp.shape}\")\n","\n","    except Exception as e:\n","        print(f\"Error loading {os.path.basename(file_path)}: {e}\")\n","\n","# Concatenate all DataFrames in the list into one master DataFrame\n","df_master = pd.concat(list_of_dfs, ignore_index=True)\n","\n","print(\"\\n--- Concatenation Complete ---\")\n","print(f\"Final shape of the raw master DataFrame: {df_master.shape}\")\n","\n","# --- 2. Data Clean-up (Crucial for CIC-IDS-2017) ---\n","\n","# A. Clean up column names: remove leading/trailing spaces\n","df_master.columns = df_master.columns.str.strip()\n","\n","# B. Handle Infinity and NaN values\n","# The dataset contains 'Infinity' values, which can cause issues in model training.\n","# Replace infinite values with NaN\n","df_master.replace([np.inf, -np.inf], np.nan, inplace=True)\n","\n","# Fill any remaining NaN values (often caused by the infinity replacement) with 0.\n","# You might choose a different imputation strategy later, but 0 is a common starting point.\n","df_master.fillna(0, inplace=True)\n","\n","print(\"\\n--- Data Clean-up Complete ---\")\n","print(f\"Number of missing values (NaN/Inf) remaining: {df_master.isna().sum().sum()}\")\n","print(f\"Final shape of the cleaned master DataFrame: {df_master.shape}\")\n","print(\"\\nFirst 5 rows of the combined DataFrame:\")\n","print(df_master.head())"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":533,"status":"ok","timestamp":1759568431320,"user":{"displayName":"bt23103127 Aniket Harit","userId":"13209463969149351825"},"user_tz":-330},"id":"NhUiR-JHcOji","outputId":"4ab87c01-5cfc-4128-f42c-bb7b0081ee23"},"outputs":[{"output_type":"stream","name":"stdout","text":["Original unique labels: ['BENIGN' 'DoS slowloris' 'DoS Slowhttptest' 'DoS Hulk' 'DoS GoldenEye'\n"," 'Heartbleed' 'Web Attack � Brute Force' 'Web Attack � XSS'\n"," 'Web Attack � Sql Injection' 'FTP-Patator' 'SSH-Patator' 'DDoS'\n"," 'PortScan' 'Bot' 'Infiltration']\n","Encoded labels: [ 0  6  5  4  3  8 12 14 13  7 11  2 10  1  9]\n","\n","Label Mapping: {'BENIGN': np.int64(0), 'Bot': np.int64(1), 'DDoS': np.int64(2), 'DoS GoldenEye': np.int64(3), 'DoS Hulk': np.int64(4), 'DoS Slowhttptest': np.int64(5), 'DoS slowloris': np.int64(6), 'FTP-Patator': np.int64(7), 'Heartbleed': np.int64(8), 'Infiltration': np.int64(9), 'PortScan': np.int64(10), 'SSH-Patator': np.int64(11), 'Web Attack � Brute Force': np.int64(12), 'Web Attack � Sql Injection': np.int64(13), 'Web Attack � XSS': np.int64(14)}\n"]}],"source":["from sklearn.preprocessing import LabelEncoder\n","\n","# 1. Non-Target Categorical Features (e.g., 'Protocol')\n","# Let's inspect data types to confirm the categorical columns.\n","# We'll stick to 'Protocol' as an example for one-hot encoding if it's present and non-numeric.\n","\n","# You can run: print(df_master.dtypes) to confirm which columns are 'object' type.\n","\n","# One-hot encode the 'Protocol' column (if it exists and is an object type)\n","# Note: For this dataset, 'Protocol' is often already numeric, but we include this step as a general practice.\n","if 'Protocol' in df_master.columns and df_master['Protocol'].dtype == 'object':\n","    df_master = pd.get_dummies(df_master, columns=['Protocol'], prefix='Protocol')\n","    print(\"One-hot encoded 'Protocol' column.\")\n","\n","# 2. Target Label Encoding\n","# The 'Label' column contains attack types (Benign, DDoS, etc.)\n","# We must convert these strings into integers.\n","\n","# Ensure the column name is correct (it was 'Label' after stripping whitespace)\n","label_column = 'Label'\n","\n","le = LabelEncoder()\n","df_master[label_column + '_encoded'] = le.fit_transform(df_master[label_column])\n","\n","print(f\"Original unique labels: {df_master[label_column].unique()}\")\n","print(f\"Encoded labels: {df_master[label_column + '_encoded'].unique()}\")\n","\n","# You can save the mapping to understand which number corresponds to which attack\n","label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n","print(\"\\nLabel Mapping:\", label_mapping)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6705,"status":"ok","timestamp":1759568438045,"user":{"displayName":"bt23103127 Aniket Harit","userId":"13209463969149351825"},"user_tz":-330},"id":"PRhKf-vUcvZD","outputId":"d3d6d8c0-6e51-490f-8be5-8178a20f2bf0"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Dataset Splitting Complete ---\n","X_train shape: (1981520, 78)\n","X_test shape: (849223, 78)\n","y_train shape: (1981520,)\n","y_test shape: (849223,)\n"]}],"source":["from sklearn.model_selection import train_test_split\n","\n","# Define the Features (X) and the Target (y)\n","# X is all columns EXCEPT the original 'Label' column and the new target column.\n","# y is the new numerical target column.\n","\n","# Drop the original string label column as it's no longer needed\n","X = df_master.drop([label_column, label_column + '_encoded'], axis=1)\n","\n","# Set the encoded label as the target variable\n","y = df_master[label_column + '_encoded']\n","\n","# Split the data into training and testing sets (e.g., 70% train, 30% test)\n","# random_state ensures reproducibility\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.3, random_state=42, stratify=y\n",")\n","\n","print(\"\\n--- Dataset Splitting Complete ---\")\n","print(f\"X_train shape: {X_train.shape}\")\n","print(f\"X_test shape: {X_test.shape}\")\n","print(f\"y_train shape: {y_train.shape}\")\n","print(f\"y_test shape: {y_test.shape}\")"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4228,"status":"ok","timestamp":1759568442287,"user":{"displayName":"bt23103127 Aniket Harit","userId":"13209463969149351825"},"user_tz":-330},"id":"-kyBSKgEdK_m","outputId":"40610963-2926-421f-8215-a17018a659ad"},"outputs":[{"output_type":"stream","name":"stdout","text":["X_train scaled successfully.\n","X_test scaled successfully.\n","\n","--- Scaling Complete ---\n","X_train_scaled shape: (1981520, 78)\n","X_test_scaled shape: (849223, 78)\n"]}],"source":["from sklearn.preprocessing import StandardScaler\n","\n","# 1. Initialize the StandardScaler\n","# This calculates the mean and standard deviation ONLY from the training data.\n","scaler = StandardScaler()\n","\n","# 2. Fit the scaler on the training data and transform it\n","X_train_scaled = scaler.fit_transform(X_train)\n","print(\"X_train scaled successfully.\")\n","\n","# 3. Transform the test data using the *fitted* scaler\n","# DO NOT call .fit() on the test data!\n","X_test_scaled = scaler.transform(X_test)\n","print(\"X_test scaled successfully.\")\n","\n","# Convert back to DataFrame for better usability/column reference (optional but recommended)\n","X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n","X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n","\n","print(\"\\n--- Scaling Complete ---\")\n","print(f\"X_train_scaled shape: {X_train_scaled.shape}\")\n","print(f\"X_test_scaled shape: {X_test_scaled.shape}\")"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":389,"status":"ok","timestamp":1759568442690,"user":{"displayName":"bt23103127 Aniket Harit","userId":"13209463969149351825"},"user_tz":-330},"id":"LpoMUIJsdjdr"},"outputs":[],"source":["from sklearn.metrics import accuracy_score, classification_report\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC\n","import xgboost as xgb\n","import time # To measure training time"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":709556,"status":"ok","timestamp":1759564974057,"user":{"displayName":"bt23103127 Aniket Harit","userId":"13209463969149351825"},"user_tz":-330},"id":"d1tx60pvd09s","outputId":"46f21c98-6694-4dc6-db18-e4960263438c"},"outputs":[{"output_type":"stream","name":"stdout","text":["--- Training XGBoost Classifier ---\n","Training Time: 685.24 seconds\n","\n","Evaluation:\n","Accuracy: 0.9990\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00    681929\n","           1       0.95      0.76      0.84       590\n","           2       1.00      1.00      1.00     38408\n","           3       1.00      1.00      1.00      3088\n","           4       1.00      1.00      1.00     69322\n","           5       0.99      0.99      0.99      1650\n","           6       0.99      1.00      1.00      1739\n","           7       1.00      1.00      1.00      2381\n","           8       1.00      1.00      1.00         3\n","           9       1.00      0.73      0.84        11\n","          10       0.99      1.00      1.00     47679\n","          11       1.00      1.00      1.00      1769\n","          12       0.75      0.88      0.81       452\n","          13       0.75      0.50      0.60         6\n","          14       0.53      0.32      0.39       196\n","\n","    accuracy                           1.00    849223\n","   macro avg       0.93      0.88      0.90    849223\n","weighted avg       1.00      1.00      1.00    849223\n","\n"]}],"source":["print(\"--- Training XGBoost Classifier ---\")\n","\n","# Use unscaled data (X_train) as tree-based models are scale-invariant\n","X_train_xgb = X_train\n","X_test_xgb = X_test\n","\n","start_time = time.time()\n","xgb_model = xgb.XGBClassifier(\n","    objective='multi:softmax', # For multi-class classification\n","    num_class=len(y_train.unique()),\n","    n_estimators=100, # Number of boosting rounds (trees)\n","    random_state=42,\n","    eval_metric='mlogloss'\n",")\n","\n","# Train the model\n","xgb_model.fit(X_train_xgb, y_train)\n","training_time = time.time() - start_time\n","print(f\"Training Time: {training_time:.2f} seconds\")\n","\n","# Make predictions on the test set\n","y_pred_xgb = xgb_model.predict(X_test_xgb)\n","\n","# Evaluate\n","print(\"\\nEvaluation:\")\n","print(f\"Accuracy: {accuracy_score(y_test, y_pred_xgb):.4f}\")\n","print(classification_report(y_test, y_pred_xgb, zero_division=0))"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"5oflVPpnd4lX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1759566676881,"user_tz":-330,"elapsed":1322816,"user":{"displayName":"bt23103127 Aniket Harit","userId":"13209463969149351825"}},"outputId":"ecc7ba82-2b94-4d88-b023-c590c298b860"},"outputs":[{"output_type":"stream","name":"stdout","text":["--- Training Random Forest Classifier ---\n","Training Time: 1306.77 seconds\n","\n","Evaluation:\n","Accuracy: 0.9985\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00    681929\n","           1       0.90      0.76      0.82       590\n","           2       1.00      1.00      1.00     38408\n","           3       1.00      1.00      1.00      3088\n","           4       1.00      1.00      1.00     69322\n","           5       0.99      0.99      0.99      1650\n","           6       0.99      1.00      0.99      1739\n","           7       1.00      1.00      1.00      2381\n","           8       1.00      1.00      1.00         3\n","           9       1.00      0.73      0.84        11\n","          10       0.99      0.99      0.99     47679\n","          11       1.00      1.00      1.00      1769\n","          12       0.75      0.83      0.79       452\n","          13       0.50      0.33      0.40         6\n","          14       0.46      0.32      0.37       196\n","\n","    accuracy                           1.00    849223\n","   macro avg       0.91      0.86      0.88    849223\n","weighted avg       1.00      1.00      1.00    849223\n","\n"]}],"source":["print(\"--- Training Random Forest Classifier ---\")\n","\n","# Use unscaled data (X_train)\n","X_train_rf = X_train\n","X_test_rf = X_test\n","\n","start_time = time.time()\n","rf_model = RandomForestClassifier(\n","    n_estimators=100, # Number of trees in the forest\n","    random_state=42,\n","    n_jobs=-1 # Use all available cores for speed\n",")\n","\n","# Train the model\n","rf_model.fit(X_train_rf, y_train)\n","training_time = time.time() - start_time\n","print(f\"Training Time: {training_time:.2f} seconds\")\n","\n","# Make predictions\n","y_pred_rf = rf_model.predict(X_test_rf)\n","\n","# Evaluate\n","print(\"\\nEvaluation:\")\n","print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n","print(classification_report(y_test, y_pred_rf, zero_division=0))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":56766,"status":"ok","timestamp":1759135343807,"user":{"displayName":"bt23103127 Aniket Harit","userId":"13209463969149351825"},"user_tz":-330},"id":"XXpsMui2hYbz","outputId":"558e1b1b-800e-4b89-c8f3-5309971ebf32"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","All models saved successfully to your Google Drive!\n"]}],"source":["import joblib\n","import os\n","\n","# --- 1. Mount Google Drive (Required for Colab) ---\n","# If you are still in Colab, you need to re-run this:\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Define the folder path in your Google Drive where you want to save the models\n","SAVE_DIR = '/content/drive/MyDrive/IDS_Models/'\n","os.makedirs(SAVE_DIR, exist_ok=True)\n","\n","# --- 2. Save Each Model ---\n","\n","# Save XGBoost\n","joblib.dump(xgb_model, os.path.join(SAVE_DIR, 'xgb_cicids2017.joblib'))\n","\n","\n","print(\"All models saved successfully to your Google Drive!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1486,"status":"ok","timestamp":1759135381862,"user":{"displayName":"bt23103127 Aniket Harit","userId":"13209463969149351825"},"user_tz":-330},"id":"LcRlewmxlFcj","outputId":"7e0f5050-8ed9-49ed-a78d-989908e39398"},"outputs":[{"data":{"text/plain":["['/content/drive/MyDrive/IDS_Models/rf_cicids2017.joblib']"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["joblib.dump(rf_model, os.path.join(SAVE_DIR, 'rf_cicids2017.joblib'))\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WCl4OJWho6_M","outputId":"52ccc00f-78e2-4b64-ce53-c0eb9df069a2","executionInfo":{"status":"ok","timestamp":1759568207757,"user_tz":-330,"elapsed":3104,"user":{"displayName":"bt23103127 Aniket Harit","userId":"13209463969149351825"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["X and y are not defined. Please run the data splitting cell (PRhKf-vUcvZD) first.\n"]}],"source":["from sklearn.utils import resample\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","import pandas as pd # Ensure pandas is imported\n","import numpy as np # Ensure numpy is imported\n","\n","\n","# --- Step 1: Convert to float32 to save memory ---\n","# This is already done globally for X in a previous cell, but ensuring here.\n","# X is defined in cell PRhKf-vUcvZD\n","\n","# Combine X + y for sampling\n","# Ensure X and y are defined before this step.\n","# X is defined in cell PRhKf-vUcvZD\n","# y is defined in cell PRhKf-vUcvZD\n","if 'X' not in locals() or 'y' not in locals():\n","    print(\"X and y are not defined. Please run the data splitting cell (PRhKf-vUcvZD) first.\")\n","else:\n","    # Convert to float32 to save memory\n","    X_temp = X.astype(np.float32)\n","\n","    # Combine X + y for sampling\n","    df_small = pd.concat([X_temp, y], axis=1)\n","\n","    # --- Step 2: Sample per class ---\n","    # Define the desired sample size per class\n","    sample_size_per_class = 50000 # You can adjust this value (e.g., 10000, 5000, etc.)\n","\n","    df_balanced = df_small.groupby(y.name, group_keys=False).apply(\n","        lambda x: x.sample(n=min(len(x), sample_size_per_class), random_state=42)\n","    )\n","\n","    print(\"Balanced subset shape:\", df_balanced.shape)\n","    print(\"Class distribution:\\n\", df_balanced[y.name].value_counts())\n","\n","    # Separate back into X, y\n","    X_bal = df_balanced.drop(columns=[y.name])\n","    y_bal = df_balanced[y.name]\n","\n","    # --- Step 3: Train/test split ---\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        X_bal, y_bal, test_size=0.3, stratify=y_bal, random_state=42\n","    )\n","\n","    # --- Step 4: Scaling ---\n","    scaler = StandardScaler()\n","    X_train_scaled = scaler.fit_transform(X_train)\n","    X_test_scaled = scaler.transform(X_test)\n","\n","    # --- Step 5: Train KNN ---\n","    # This part is for KNN training and can be kept or removed depending on your focus.\n","    # If you only want to run SVM, you can comment out or remove this section.\n","    # knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n","    # print(\"\\n--- Training KNN on Sampled Balanced Subset ---\")\n","    # knn.fit(X_train_scaled, y_train)\n","\n","    # y_pred = knn.predict(X_test_scaled)\n","\n","    # --- Step 6: Evaluation ---\n","    # This part is for KNN evaluation and can be kept or removed.\n","    # print(\"\\n--- Classification Report ---\")\n","    # print(classification_report(y_test, y_pred, target_names=le.classes_))\n","\n","    # print(\"\\nConfusion Matrix:\")\n","    # print(confusion_matrix(y_test, y_pred))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jlheTtUopKzq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1759518363352,"user_tz":-330,"elapsed":207,"user":{"displayName":"bt23103127 Aniket Harit","userId":"13209463969149351825"}},"outputId":"429bc9d9-8195-4c0e-a08f-b88f05cf2ebe"},"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Model saved successfully!\n"]}],"source":["import os\n","import joblib\n","\n","# Set a save directory in your Google Drive\n","SAVE_DIR = \"/content/drive/MyDrive/models\"\n","os.makedirs(SAVE_DIR, exist_ok=True)\n","\n","# Suppose your trained model is `knn` (or `rf_model` if RandomForest)\n","joblib.dump(knn, os.path.join(SAVE_DIR, \"knn_cicids2017.joblib\"))\n","\n","print(\"✅ Model saved successfully!\")\n"]},{"cell_type":"code","source":["# ============================\n","# 🧠 Lightweight SVM Pipeline\n","# ============================\n","\n","import os\n","import joblib\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import LinearSVC\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","# ====== 1️⃣ Sampling (safe for Colab) ======\n","# Limit to 2,000 samples per class\n","svm_sample = (\n","    df_master.groupby(\"Label_encoded\", group_keys=False)\n","    .apply(lambda x: x.sample(min(len(x), 2000), random_state=42))\n","    .reset_index(drop=True)\n",")\n","\n","print(f\"Sampled {len(svm_sample)} rows for SVM training.\")\n","\n","# ====== 2️⃣ Feature/Target split ======\n","X_svm = svm_sample.drop([\"Label\", \"Label_encoded\"], axis=1)\n","y_svm = svm_sample[\"Label_encoded\"]\n","\n","# ====== 3️⃣ Train/Test split ======\n","X_train_svm, X_test_svm, y_train_svm, y_test_svm = train_test_split(\n","    X_svm, y_svm, test_size=0.3, random_state=42, stratify=y_svm\n",")\n","\n","# ====== 4️⃣ Scale features ======\n","scaler_svm = StandardScaler()\n","X_train_svm = scaler_svm.fit_transform(X_train_svm.astype(\"float32\"))\n","X_test_svm = scaler_svm.transform(X_test_svm.astype(\"float32\"))\n","\n","# ====== 5️⃣ Train Linear SVM ======\n","svm_model = LinearSVC(C=1.0, max_iter=2000, random_state=42)\n","svm_model.fit(X_train_svm, y_train_svm)\n","\n","# ====== 6️⃣ Evaluate ======\n","y_pred_svm = svm_model.predict(X_test_svm)\n","print(\"\\n✅ SVM Evaluation Results:\")\n","print(\"Accuracy:\", accuracy_score(y_test_svm, y_pred_svm))\n","print(classification_report(y_test_svm, y_pred_svm))\n","\n","# ====== 7️⃣ Save model to Drive ======\n","SAVE_DIR = \"/content/drive/MyDrive/IDS_Models\"\n","os.makedirs(SAVE_DIR, exist_ok=True)\n","\n","joblib.dump(svm_model, os.path.join(SAVE_DIR, \"linear_svm_cicids2017.joblib\"))\n","joblib.dump(scaler_svm, os.path.join(SAVE_DIR, \"linear_svm_scaler.joblib\"))\n","\n","print(f\"\\n📁 Model and scaler saved to: {SAVE_DIR}\")\n"],"metadata":{"id":"ra9aSKWbdsaJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1759568548139,"user_tz":-330,"elapsed":105433,"user":{"displayName":"bt23103127 Aniket Harit","userId":"13209463969149351825"}},"outputId":"22a89318-3564-42ee-9663-a591277da09e"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3995657491.py:17: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  .apply(lambda x: x.sample(min(len(x), 2000), random_state=42))\n"]},{"output_type":"stream","name":"stdout","text":["Sampled 22193 rows for SVM training.\n","\n","✅ SVM Evaluation Results:\n","Accuracy: 0.9321117452688495\n","              precision    recall  f1-score   support\n","\n","           0       0.96      0.78      0.86       600\n","           1       0.90      0.98      0.94       590\n","           2       0.98      1.00      0.99       600\n","           3       0.99      0.97      0.98       600\n","           4       0.95      0.99      0.97       600\n","           5       0.96      0.99      0.97       600\n","           6       0.96      0.96      0.96       600\n","           7       0.98      0.99      0.99       600\n","           8       0.75      1.00      0.86         3\n","           9       0.83      0.45      0.59        11\n","          10       0.98      1.00      0.99       600\n","          11       0.91      0.99      0.95       600\n","          12       0.68      0.91      0.78       452\n","          13       0.00      0.00      0.00         6\n","          14       1.00      0.03      0.05       196\n","\n","    accuracy                           0.93      6658\n","   macro avg       0.86      0.80      0.79      6658\n","weighted avg       0.94      0.93      0.92      6658\n","\n","\n","📁 Model and scaler saved to: /content/drive/MyDrive/IDS_Models\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]}]},{"cell_type":"code","source":["# ============================\n","# ⚙️ Logistic Regression Model\n","# ============================\n","\n","import os\n","import joblib\n","import pandas as pd\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, accuracy_score\n","\n","# ====== 1️⃣ Sampling (reuse SVM strategy) ======\n","logit_sample = (\n","    df_master.groupby(\"Label_encoded\", group_keys=False)\n","    .apply(lambda x: x.sample(min(len(x), 2000), random_state=42))\n","    .reset_index(drop=True)\n",")\n","\n","print(f\"Sampled {len(logit_sample)} rows for Logistic Regression.\")\n","\n","# ====== 2️⃣ Features & Target ======\n","X_logit = logit_sample.drop([\"Label\", \"Label_encoded\"], axis=1)\n","y_logit = logit_sample[\"Label_encoded\"]\n","\n","# ====== 3️⃣ Train/Test split ======\n","X_train_logit, X_test_logit, y_train_logit, y_test_logit = train_test_split(\n","    X_logit, y_logit, test_size=0.3, random_state=42, stratify=y_logit\n",")\n","\n","# ====== 4️⃣ Scale the data ======\n","scaler_logit = StandardScaler()\n","X_train_logit = scaler_logit.fit_transform(X_train_logit.astype(\"float32\"))\n","X_test_logit = scaler_logit.transform(X_test_logit.astype(\"float32\"))\n","\n","# ====== 5️⃣ Train Logistic Regression ======\n","# 'lbfgs' solver supports multinomial (multiclass) classification\n","logit_model = LogisticRegression(\n","    multi_class=\"multinomial\", solver=\"lbfgs\", max_iter=500, n_jobs=-1\n",")\n","logit_model.fit(X_train_logit, y_train_logit)\n","\n","# ====== 6️⃣ Evaluate ======\n","y_pred_logit = logit_model.predict(X_test_logit)\n","\n","print(\"\\n✅ Logistic Regression Evaluation Results:\")\n","print(\"Accuracy:\", accuracy_score(y_test_logit, y_pred_logit))\n","print(classification_report(y_test_logit, y_pred_logit))\n","\n","# ====== 7️⃣ Save model & scaler ======\n","SAVE_DIR = \"/content/drive/MyDrive/IDS_Models\"\n","os.makedirs(SAVE_DIR, exist_ok=True)\n","\n","joblib.dump(logit_model, os.path.join(SAVE_DIR, \"logistic_cicids2017.joblib\"))\n","joblib.dump(scaler_logit, os.path.join(SAVE_DIR, \"logistic_scaler.joblib\"))\n","\n","print(f\"\\n📁 Model and scaler saved to: {SAVE_DIR}\")\n"],"metadata":{"id":"U5kAcFyue_Si","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1759570101137,"user_tz":-330,"elapsed":16849,"user":{"displayName":"bt23103127 Aniket Harit","userId":"13209463969149351825"}},"outputId":"def1a100-de29-4c00-88ff-d4be5cf302b7"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Sampled 22193 rows for Logistic Regression.\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1321569212.py:16: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  .apply(lambda x: x.sample(min(len(x), 2000), random_state=42))\n","/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","✅ Logistic Regression Evaluation Results:\n","Accuracy: 0.9294082306999099\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.80      0.88       600\n","           1       0.91      0.98      0.95       590\n","           2       0.98      1.00      0.99       600\n","           3       0.99      0.97      0.98       600\n","           4       0.94      0.99      0.97       600\n","           5       0.97      0.94      0.96       600\n","           6       0.94      0.96      0.95       600\n","           7       0.98      0.99      0.99       600\n","           8       1.00      1.00      1.00         3\n","           9       0.86      0.55      0.67        11\n","          10       0.97      0.99      0.98       600\n","          11       0.90      0.99      0.94       600\n","          12       0.68      0.91      0.78       452\n","          13       0.00      0.00      0.00         6\n","          14       0.71      0.03      0.05       196\n","\n","    accuracy                           0.93      6658\n","   macro avg       0.85      0.81      0.80      6658\n","weighted avg       0.93      0.93      0.92      6658\n","\n","\n","📁 Model and scaler saved to: /content/drive/MyDrive/IDS_Models\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"lDI9zyu-jL_f"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPzW+5/bqKF0xRp0i3HBTPD"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}